<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="A benchmark for detecting LLM biases on NLU tasks in AAVE.">
  <meta name="keywords" content="AAVENUE, Benchmark, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .shiny-button {
      background: #333;
      border: 1px solid #444;
      border-radius: 5px;
      color: #fff;
      cursor: pointer;
      display: inline-flex;
      align-items: center;
      font-size: 16px;
      padding: 10px 20px;
      text-align: center;
      text-decoration: none;
      position: relative;
      overflow: hidden;
      transition: all 0.3s ease;
      font-family: Arial, sans-serif;
    }

    .shiny-button .icon {
      margin-right: 8px;
    }

    .shiny-button::before {
      content: '';
      position: absolute;
      top: 50%;
      left: 50%;
      width: 300%;
      height: 300%;
      background: radial-gradient(circle, rgba(255, 255, 255, 0.2) 0%, rgba(255, 255, 255, 0) 60%);
      transition: all 0.3s ease;
      transform: translate(-50%, -50%) scale(0);
      border-radius: 50%;
      z-index: 0;
    }

    .shiny-button:hover::before {
      transform: translate(-50%, -50%) scale(1);
    }

    .shiny-button span {
      position: relative;
      z-index: 1;
    }
  </style>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="http://aavenue.live/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
      </div>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/abhaygupta12/">Abhay Gupta</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/philip-meng-1402b0258/">Philip Meng</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ece-yurtseven-b61b68259/">Ece Yurtseven</a><sup>3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>John Jay Senior High School,</span>
              <span class="author-block"><sup>2</sup>Phillips Academy,</span>
              <span class="author-block"><sup>3</sup>Robert College</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/abhayg1266/AAVENUE-Code" class="shiny-button">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

              <section class="section">
                <div class="container is-max-desktop">
                  <!-- Abstract. -->
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                      <h2 class="title is-3">Abstract</h2>
                      <div class="content has-text-justified">
                        <p>
                          Detecting biases in natural language under
                          standing (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language process
                          ing (NLP) systems. To address dialect
                          induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language
                          Understanding Evaluation), a benchmark for
                          evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard
                          American English (SAE). AAVENUE builds
                          upon and extends existing benchmarks like
                          VALUE, replacing deterministic syntactic and
                          morphological transformations with a more
                          flexible methodology leveraging LLM-based
                          translation with few-shot prompting, improving
                          performance across several evaluation metrics
                          when translating key tasks from the GLUE and
                          SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics
                          including fluency, BARTScore, quality, coherence, and understandability. Additionally, the
                          fluency of AAVENUE is validated by annotations from AAVE speakers. Our evaluations
                          reveal that LLMs consistently perform better
                          on SAE tasks than AAVE-translated versions,
                          underscoring inherent biases and highlighting
                          the need for more inclusive NLP models
                         </p>
                      </div>
                    </div>
                  </div>
                  <!--/ Abstract. -->
                </div>
              </section>

              <!-- Methodology Section -->
              <section class="section">
                <div class="container is-max-desktop">
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                      <h2 class="title is-4">Methodology & Data</h2>
                      <div class="content has-text-justified">
                        <p>
                          The AAVENUE benchmark is designed to evaluate LLM performance on NLU tasks across AAVE and SAE. We extended existing benchmarks by leveraging few-shot prompted translations using GPT-4o-mini, enhancing flexibility compared to deterministic linguistic transformations used by benchmarks like VALUE. Below, we explain our task selection, translation process, validation, and evaluation metrics.
                        </p>
              
                        <h3 class="title is-5">Task Selection</h3>
                        <p>
                          We selected five key tasks from the GLUE and SuperGLUE benchmarks to test model performance in SAE and AAVE. The chosen tasks assess different aspects of NLU:
                        </p>
              
                        <!-- Task Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>Description</th>
                              <th>Aspect Tested</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>BoolQ</td>
                              <td>Yes/no questions based on a passage.</td>
                              <td>Comprehension and information processing.</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>Answering questions requiring connection of information across a passage.</td>
                              <td>Handling complex and interconnected texts.</td>
                            </tr>
                            <tr>
                              <td>SST-2</td>
                              <td>Sentiment analysis of movie reviews.</td>
                              <td>Understanding sentiment in different dialects.</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>Choosing the most plausible outcome or cause from two alternatives.</td>
                              <td>Cause-and-effect reasoning.</td>
                            </tr>
                            <tr>
                              <td>WSC</td>
                              <td>Determining which noun a pronoun refers to in ambiguous contexts.</td>
                              <td>Pronoun resolution and dialectal nuances.</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <h3 class="title is-5">SAE to AAVE Translation</h3>
                        <p>
                          We used GPT-4o-mini with few-shot prompting to translate 1000 data points for each task from SAE to AAVE. Below are examples of translations for each task:
                        </p>
              
                        <!-- Translation Examples Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>SAE Example</th>
                              <th>AAVE Example</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>SST-2</td>
                              <td>The movie was preachy and poorly acted.</td>
                              <td>Ain't much to like, it be preachy and acted bad.</td>
                            </tr>
                            <tr>
                              <td>BoolQ</td>
                              <td>Can I be sacked for falling asleep at work?</td>
                              <td>Can I get fired for fallin' asleep on the job?</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>Man lost the competition. Choice 1: The competition was sabotaged. Choice 2: He intimidated his competitors. (Selected: Choice 1)</td>
                              <td>Man lost da competition. Choice 1: Da competition got messed up. Choice 2: He scared off his competitors. (Selected: Choice 1)</td>
                            </tr>
                            <tr>
                              <td>WSC</td>
                              <td>Sam Goodman's biography of the Spartan general Xenophanes shows the difficulties he faced in his childhood.</td>
                              <td>Sam Goodman's biography on that Spartan general Xenophanes show y'all the tough times he had growin' up.</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>Paragraph: A stranger in town... How does Jason react to the stranger's presence?</td>
                              <td>Paragraph: A stranger in town... How Jason be actin' to that stranger around?</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <h3 class="title is-5">Validation and Evaluation Metrics</h3>
                        <p>
                          We validated our translations using several key metrics. The results from our evaluations are summarized below:
                        </p>
              
                        <!-- Validation Metrics Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Metric</th>
                              <th>Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Fluency</td>
                              <td>Evaluates the grammatical correctness and natural flow of the text (scored out of 100).</td>
                            </tr>
                            <tr>
                              <td>Coherence</td>
                              <td>Assesses the logical flow and consistency of the text (scored out of 100).</td>
                            </tr>
                            <tr>
                              <td>Understandability</td>
                              <td>Determines how easily the translation can be understood (scored out of 100).</td>
                            </tr>
                            <tr>
                              <td>Quality</td>
                              <td>Overall quality assessment of the translation (scored out of 100).</td>
                            </tr>
                            <tr>
                              <td>BARTScore</td>
                              <td>Measures how closely the AAVE translation aligns with the original SAE sentence, with lower scores indicating better alignment.</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <h3 class="title is-5">Human Validation</h3>
                        <p>
                          We recruited 10 fluent AAVE speakers from the Bronx and Queens, NY, to evaluate the cultural and linguistic authenticity of the AAVE translations. Each translation was rated on a scale of 1 to 10 for its accuracy in reflecting AAVE. Below are the average scores from the human validators:
                        </p>
              
                        <!-- Human Validation Scores Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>Average Score (Out of 10)</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>BoolQ</td>
                              <td>7.02</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>7.27</td>
                            </tr>
                            <tr>
                              <td>SST-2</td>
                              <td>7.09</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>7.22</td>
                            </tr>
                            <tr>
                              <td>WSC</td>
                              <td>7.25</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <h3 class="title is-5">Comparison of AAVENUE vs. VALUE</h3>
                        <p>
                          We compared the AAVENUE translations to those generated by the VALUE benchmark across several large language models (LLMs) using binary comparison tasks. Below are the results of these comparisons:
                        </p>
              
                        <!-- Comparison Scores Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>Model</th>
                              <th>AAVENUE Preference (%)</th>
                              <th>VALUE Preference (%)</th>
                              <th>About the Same (%)</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>BoolQ</td>
                              <td>GPT-4-turbo</td>
                              <td>94.51%</td>
                              <td>4.62%</td>
                              <td>0.88%</td>
                            </tr>
                            <tr>
                              <td>BoolQ</td>
                              <td>GPT-4o-mini</td>
                              <td>88.79%</td>
                              <td>10.33%</td>
                              <td>0.88%</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>GPT-4o-mini</td>
                              <td>90.42%</td>
                              <td>9.38%</td>
                              <td>0.21%</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>Gemini-1.5-flash</td>
                              <td>93.85%</td>
                              <td>6.15%</td>
                              <td>0.00%</td>
                            </tr>
                          </tbody>
                        </table>
                        
                      </div>
                    </div>
                  </div>
                </div>
              </section>

              <!--/ Methodology Section -->

              <!-- Results Section -->
              <section class="section">
                <div class="container is-max-desktop">
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                      <h2 class="title is-4">Results</h2>
                      <div class="content has-text-justified">
                        <p>
                          Our evaluations reveal significant performance differences between SAE and AAVE translations across five NLU tasks. Large language models (LLMs) consistently performed better on SAE tasks than their AAVE counterparts. Below, we summarize the accuracy scores and comparison metrics used to evaluate model performance across these tasks.
                        </p>
              
                        <h3 class="title is-5">Accuracy Scores</h3>
                        <p>
                          We evaluated the accuracy of translations across five key tasks using five popular LLMs. The following tables present the accuracy scores for GPT and Gemini models across both SAE and AAVE tasks.
                        </p>
              
                        <!-- GPT Accuracy Scores Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>GPT-4o-mini (SAE/AAVE)</th>
                              <th>GPT-4-turbo (SAE/AAVE)</th>
                              <th>GPT-4o (SAE/AAVE)</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>SST-2</td>
                              <td>90.40% / 88.40% (-2.0)</td>
                              <td>94.00% / 92.80% (-1.2)</td>
                              <td>88.80% / 87.30% (-1.5)</td>
                            </tr>
                            <tr>
                              <td>BoolQ</td>
                              <td>88.29% / 85.29% (-3.0)</td>
                              <td>88.09% / 86.49% (-1.6)</td>
                              <td>89.19% / 86.89% (-2.3)</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>95.40% / 93.20% (-2.2)</td>
                              <td>97.60% / 96.80% (-0.8)</td>
                              <td>97.20% / 96.40% (-0.8)</td>
                            </tr>
                            <tr>
                              <td>WSC</td>
                              <td>60.03% / 57.90% (-2.1)</td>
                              <td>69.60% / 68.69% (-0.9)</td>
                              <td>70.36% / 67.02% (-3.3)</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>84.50% / 72.00% (-12.5)</td>
                              <td>86.20% / 73.70% (-12.5)</td>
                              <td>87.50% / 71.30% (-16.2)</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <!-- Gemini Accuracy Scores Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>Gemini-1.5-Flash (SAE/AAVE)</th>
                              <th>Gemini-1.5-Pro (SAE/AAVE)</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>SST-2</td>
                              <td>87.70% / 87.10% (-0.6)</td>
                              <td>92.00% / 91.40% (-0.6)</td>
                            </tr>
                            <tr>
                              <td>BoolQ</td>
                              <td>89.69% / 87.29% (-2.4)</td>
                              <td>89.49% / 85.89% (-3.6)</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>91.40% / 92.00% (+0.6)</td>
                              <td>97.40% / 95.80% (-1.6)</td>
                            </tr>
                            <tr>
                              <td>WSC</td>
                              <td>48.78% / 48.48% (-0.3)</td>
                              <td>51.37% / 51.22% (-0.2)</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>84.10% / 70.70% (-13.4)</td>
                              <td>85.90% / 71.90% (-14.0)</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <h3 class="title is-5">Accuracy Score Analysis</h3>
                        <p>
                          The accuracy scores reveal consistent performance drops when models handle AAVE translations. Tasks like <b>MultiRC</b> and <b>WSC</b> showed the largest accuracy drops, indicating challenges in reading comprehension and pronoun resolution. While GPT-4-turbo exhibited relatively smaller drops, models like GPT-4o-mini struggled more with AAVE translations, particularly in contextually complex tasks. This suggests a clear need for more inclusive training data to better handle AAVE.
                        </p>
              
                        <h3 class="title is-5">Intersection Over Union (IoU) Analysis</h3>
                        <p>
                          We analyzed the intersection over union (IoU) between incorrect answers in SAE and AAVE translations to understand whether models faced similar difficulties across dialects. The following table summarizes the IoU percentages:
                        </p>
              
                        <!-- IoU Analysis Table -->
                        <table class="table is-bordered is-striped is-narrow is-fullwidth">
                          <thead>
                            <tr>
                              <th>Task</th>
                              <th>GPT-4o-mini</th>
                              <th>GPT-4-turbo</th>
                              <th>GPT-4o</th>
                              <th>Gemini-1.5-Flash</th>
                              <th>Gemini-1.5-Pro</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>SST-2</td>
                              <td>8.40%</td>
                              <td>5.10%</td>
                              <td>9.80%</td>
                              <td>10.40%</td>
                              <td>6.20%</td>
                            </tr>
                            <tr>
                              <td>BoolQ</td>
                              <td>10.21%</td>
                              <td>10.71%</td>
                              <td>8.91%</td>
                              <td>8.51%</td>
                              <td>8.41%</td>
                            </tr>
                            <tr>
                              <td>COPA</td>
                              <td>3.00%</td>
                              <td>1.60%</td>
                              <td>2.00%</td>
                              <td>5.80%</td>
                              <td>1.80%</td>
                            </tr>
                            <tr>
                              <td>WSC</td>
                              <td>35.56%</td>
                              <td>24.01%</td>
                              <td>25.68%</td>
                              <td>49.54%</td>
                              <td>44.53%</td>
                            </tr>
                            <tr>
                              <td>MultiRC</td>
                              <td>9.60%</td>
                              <td>9.00%</td>
                              <td>8.30%</td>
                              <td>9.90%</td>
                              <td>7.90%</td>
                            </tr>
                          </tbody>
                        </table>
              
                        <p>
                          Our IoU analysis indicates that the challenges in handling AAVE are often dialect-specific. The IoU scores reveal minimal overlap in incorrect responses between SAE and AAVE, suggesting that models encounter distinct challenges when processing AAVE texts. However, tasks like <b>WSC</b> showed significant overlap, indicating difficulty in handling pronoun resolution in both dialects.
                      </div>
                    </div>
                  </div>
                </div>
              </section>
              <!--/ Results Section -->

              <!-- Limitations Section -->
              <section class="section">
                <div class="container is-max-desktop">
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                      <h2 class="title is-4">Limitations</h2>
                      <div class="content has-text-justified">
                        <p>
                          While AAVENUE presents a comprehensive benchmark for evaluating large language models (LLMs) across Standard American English (SAE) and African American Vernacular English (AAVE), it is important to acknowledge several limitations. First, our benchmark focuses primarily on a select number of tasks from the GLUE and SuperGLUE benchmarks, which may not fully capture the broad range of real-world applications where dialectal differences play a role. Additionally, AAVE itself varies across regions and communities, and while we validated our translations with fluent AAVE speakers—who were compensated for their contributions—the inherent variability in AAVE may limit the generalizability of our findings.
                        </p>
              
                        <p>
                          Another limitation is our reliance on GPT-4o-mini for translations, which, despite being an advanced model, may still reflect biases present in its pre-training data. This reliance on a single model restricts the diversity of approaches we could explore for reducing translation biases. Furthermore, AAVENUE currently focuses exclusively on AAVE and SAE, leaving out other underrepresented dialects that could benefit from similar evaluations. Expanding the benchmark to include a wider range of dialects would provide a more complete picture of LLM inclusivity.
                        </p>
              
                        <p>
                          Lastly, while we employed various quantitative metrics such as fluency and coherence to evaluate the translations, a deeper qualitative analysis involving AAVE speakers could offer better insights into the cultural and linguistic nuances that automated metrics might overlook. These limitations highlight important areas for future research and development in ensuring more equitable and representative natural language processing systems.
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </section>
              <section class="section">
                <div class="container is-max-desktop">
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                      <h2 class="title is-4">Ethical Considerations</h2>
                      <div class="content has-text-justified">
                        <p>
                          Our research is guided by ethical principles aimed at promoting fairness and inclusivity, particularly in evaluating and addressing dialectal biases in large language models (LLMs). To ensure cultural and linguistic authenticity, we collected original data and recruited fluent AAVE speakers to validate our translations. All participants provided informed consent, and the AAVE speakers were compensated for their time and contributions.
                        </p>
              
                        <p>
                          We took careful steps to avoid potential harm and bias throughout the research process. The data collection and evaluation processes adhered to ethical guidelines, and we prioritized transparency in reporting our findings. Our goal is to contribute to the development of more inclusive natural language processing (NLP) systems that serve underrepresented dialects effectively.
                        </p>
              
                        <p>
                          By making our code and evaluation methods publicly available, we encourage further collaboration and research in this area. We believe this transparency will support ongoing efforts to create equitable NLP systems, ensuring these technologies are fair and reliable across diverse linguistic communities.
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </section>
              <!-- Related Works Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Related Works</h2>
        <div class="content has-text-justified">
          <p>
            Our work builds upon several important benchmarks and studies in the field of natural language processing. The <strong><a href="https://arxiv.org/abs/1804.07461" target="_blank">GLUE</a></strong> benchmark provides a platform for evaluating language model performance across a variety of standard linguistic tasks, primarily focusing on Standard American English (SAE). <strong><a href="https://arxiv.org/abs/1905.00537" target="_blank">SuperGLUE</a></strong> extends this by introducing more challenging tasks that require nuanced understanding and reasoning.
          </p>
          <p>
            <strong><a href="https://aclanthology.org/2022.acl-long.258" target="_blank">VALUE</a></strong> (VernAcular Language Understanding Evaluation) addresses dialect disparity in natural language understanding by using a set of linguistic transformation rules to evaluate models on African American Vernacular English (AAVE). However, its deterministic approach can limit generalizability across different contexts.
          </p>
          <p>
            Additionally, studies such as "<a href="https://aclanthology.org/P19-1163" target="_blank">The Risk of Racial Bias in Hate Speech Detection</a>" by Sap et al. and "<a href="https://arxiv.org/abs/2005.14050" target="_blank">Language (Technology) is Power: A Critical Survey of 'Bias' in NLP</a>" by Blodgett et al. have highlighted the biases that exist in language technologies, underscoring the importance of addressing these issues to develop fair and equitable NLP systems.
          </p>
          <p>
            Our benchmark, AAVENUE, seeks to expand upon these foundations by providing a more comprehensive evaluation of dialectal bias, specifically focusing on the performance of large language models in handling tasks in AAVE compared to SAE.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- FAQ Section -->
<section class="section" id="FAQ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Frequently Asked Questions</h2>

        <!-- FAQ 1 -->
        <div class="faq-item">
          <button class="button is-fullwidth" onclick="toggleFAQ('faq1')">
            <span class="faq-question">What is AAVENUE?</span>
            <span class="icon is-small" id="faq-icon1">
              <i class="fas fa-chevron-down"></i>
            </span>
          </button>
          <div id="faq1" class="faq-answer" style="display:none;">
            <p>AAVENUE (AAVE Natural Language Understanding Evaluation) is a benchmark designed to assess how well large language models handle tasks in African American Vernacular English compared to Standard American English.</p>
          </div>
        </div>

        <!-- FAQ 2 -->
        <div class="faq-item">
          <button class="button is-fullwidth" onclick="toggleFAQ('faq2')">
            <span class="faq-question">How can AAVENUE help reduce biases in NLP?</span>
            <span class="icon is-small" id="faq-icon2">
              <i class="fas fa-chevron-down"></i>
            </span>
          </button>
          <div id="faq2" class="faq-answer" style="display:none;">
            <p>By providing a detailed evaluation of language models on AAVE, AAVENUE helps identify and mitigate biases that might otherwise lead to less effective NLP applications for users of this dialect.</p>
          </div>
        </div>

        <!-- FAQ 3 -->
        <div class="faq-item">
          <button class="button is-fullwidth" onclick="toggleFAQ('faq3')">
            <span class="faq-question">Where can I access the AAVENUE dataset and tools?</span>
            <span class="icon is-small" id="faq-icon3">
              <i class="fas fa-chevron-down"></i>
            </span>
          </button>
          <div id="faq3" class="faq-answer" style="display:none;">
            <p>The tools and datasets developed as part of AAVENUE are publicly available on our GitHub repository for researchers and developers to use and contribute to.</p>
          </div>
        </div>

        <!-- FAQ 4 -->
        <div class="faq-item">
          <button class="button is-fullwidth" onclick="toggleFAQ('faq4')">
            <span class="faq-question">How does AAVENUE compare to other benchmarks?</span>
            <span class="icon is-small" id="faq-icon4">
              <i class="fas fa-chevron-down"></i>
            </span>
          </button>
          <div id="faq4" class="faq-answer" style="display:none;">
            <p>Unlike many traditional benchmarks that primarily focus on SAE, AAVENUE specifically measures performance across dialects, emphasizing the handling of AAVE to spotlight and address disparities in model performance.</p>
          </div>
        </div>

        <!-- FAQ 5 -->
        <div class="faq-item">
          <button class="button is-fullwidth" onclick="toggleFAQ('faq5')">
            <span class="faq-question">Can I contribute to the AAVENUE project?</span>
            <span class="icon is-small" id="faq-icon5">
              <i class="fas fa-chevron-down"></i>
            </span>
          </button>
          <div id="faq5" class="faq-answer" style="display:none;">
            <p>Yes, contributions are welcome! We encourage you to contribute through GitHub, whether it's improving the benchmark, offering new datasets, or providing feedback on the tool.</p>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- Footer with links and contacts -->
<footer class="footer">
  <div class="container has-text-centered">
    <p>
      <a href="https://github.com/abhayg1266/AAVENUE-Code" class="icon-link">
        <i class="fab fa-github"></i> GitHub Code
      </a>
      &nbsp;|&nbsp;
      <a href="https://drive.google.com/file/d/1IjlfozzvaAGI4q3ZmPJIy1NLgLboItHS/view?usp=sharing" class="icon-link">
        <i class="fas fa-file-pdf"></i> Paper
      </a>
    </p>
    <p>If you have any further questions, feel free to message <a href="https://www.linkedin.com/in/abhaygupta12/">Abhay Gupta</a> on LinkedIn.</p>
  </div>
</footer>
<!-- JavaScript for FAQ toggling -->
<script>
function toggleFAQ(id) {
  var element = document.getElementById(id);
  var icon = document.getElementById('faq-icon' + id.charAt(id.length-1));
  if (element.style.display === 'none') {
    element.style.display = 'block';
    icon.classList.remove('fa-chevron-down');
    icon.classList.add('fa-chevron-up');
  } else {
    element.style.display = 'none';
    icon.classList.remove('fa-chevron-up');
    icon.classList.add('fa-chevron-down');
  }
}
</script>

<div id="footer">
  © 2024 AAVENUE. All Rights Reserved
</div>
